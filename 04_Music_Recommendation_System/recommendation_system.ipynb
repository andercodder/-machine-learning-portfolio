{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Data analysis\n",
    "\n",
    "â€¢ What are the most listened songs? \\\n",
    "â€¢ Who are the most popular artists? \\\n",
    "â€¢ How is the distribution of song count for users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "song_df = pd.read_csv('song_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# find the the max play_count value of the song titles\n",
    "most_listened_songs=song_df.groupby('song',observed=True)['play_count'].sum().sort_values(ascending=False)[:10]\n",
    "result = pd.DataFrame(most_listened_songs).merge(song_df[['song', 'title']].drop_duplicates(),left_index=True,right_on='song')[['title', 'play_count']]\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the most popular song\n",
    "most_popular_song=song_df.groupby('song',observed=True)['user'].nunique().sort_values(ascending=False)[:10]\n",
    "result =pd.DataFrame(most_popular_song).merge(song_df[['song','title']],on='song',how='left').drop_duplicates().reset_index(drop=True)\n",
    "print(\"the 10 most popular songs\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the most popular artists\n",
    "most_popular_artist=song_df.groupby('artist_name',observed=True)['play_count'].sum().sort_values(ascending=False)[:10]\n",
    "print(\"the 10 most popular artists\\n\", most_popular_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the most popular artists for users\n",
    "most_popular_artist=song_df.groupby('artist_name',observed=True)['user'].nunique().sort_values(ascending=False)[:10]\n",
    "print(\"the 10 most popular artists\\n\", most_popular_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df['song'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the distribution of song count for users \n",
    "song_count=song_df.groupby('user',observed=True)['song'].count().sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(len(song_count)), song_count.values, alpha=0.5, s=10)\n",
    "plt.title('distribution')\n",
    "plt.xlabel('user index')\n",
    "plt.ylabel('number of songs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 EDA for song dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(song_df['song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = song_df['user'].nunique()\n",
    "print(user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_count = song_df['song'].nunique()\n",
    "print(song_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_count = song_df['release'].nunique()\n",
    "print(release_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = song_df['year'].unique()\n",
    "print(year_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_zero_count = song_df[song_df['year'] == 0]['year'].count()\n",
    "print(year_zero_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3 Constructing a user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of times a song has been played, maximum and minimum values\n",
    "average_play_count = song_df.groupby('song',observed=True)['play_count'].mean().mean()\n",
    "max_play_count = song_df.groupby('song',observed=True)['play_count'].max().max()\n",
    "min_play_count = song_df.groupby('song',observed=True)['play_count'].min().min()\n",
    "print(\"average play count: \", average_play_count)\n",
    "print(\"max play count: \", max_play_count)\n",
    "print(\"min play count: \", min_play_count)\n",
    "#the distribution of song play_count for songs\n",
    "song_play_count=song_df.groupby('song',observed=True)['play_count'].sum().sort_values(ascending=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(len(song_play_count)), song_play_count.values, alpha=0.5, s=10)\n",
    "plt.title('distribution of song play_count')\n",
    "plt.xlabel('song index')\n",
    "plt.ylabel('number of play_count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = pd.DataFrame()\n",
    "user_features['total_plays'] = song_df.groupby('user', observed=True)['play_count'].sum()\n",
    "user_features['unique_songs'] = song_df.groupby('user', observed=True)['song'].nunique()\n",
    "user_features['unique_artists'] = song_df.groupby('user', observed=True)['artist_name'].nunique()\n",
    "user_features['unique_release'] = song_df.groupby('user', observed=True)['release'].nunique()\n",
    "user_features['avg_plays_per_artists'] = (song_df.groupby('user', observed=True)['play_count'].sum() / \n",
    "                                        song_df.groupby('user', observed=True)['release'].nunique())\n",
    "# why not consider the yearï¼Œbecause year have too much zeros\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(user_features)\n",
    "# Use Elbow Method to find the optimal K\n",
    "from sklearn.cluster import KMeans\n",
    "inertias = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "best_k = 6 # Choose best k based on elbow plot\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "user_features['Cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n",
    "                     c=user_features['Cluster'], cmap='viridis')\n",
    "plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0],\n",
    "           pca.transform(kmeans.cluster_centers_)[:, 1], \n",
    "           marker='x', s=200, linewidths=3, color='r', label='Cluster Centers')\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('User Clustering Results')\n",
    "plt.legend( )\n",
    "plt.show()\n",
    "cluster_means = user_features.groupby('Cluster').mean()\n",
    "print(\"\\nMean Features for Each Cluster:\")\n",
    "print(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=user_features.drop('Cluster', axis=1).columns,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "\n",
    "print(\"PCA Components Weights:\")\n",
    "print(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_stats = user_features.groupby('Cluster').agg({\n",
    "    'total_plays': ['mean','sum'],\n",
    "    'unique_songs': 'mean',\n",
    "    'unique_artists': 'mean',\n",
    "    'unique_release': 'mean',\n",
    "    'avg_plays_per_artists': 'mean',\n",
    "}).round(2)\n",
    "print(\"Cluster Statistics:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "cluster_sizes = user_features['Cluster'].value_counts(normalize=True).round(3) * 100\n",
    "print(\"\\nCluster Size Percentages:\")\n",
    "print(cluster_sizes.sort_index().map(lambda x: f\"{x}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User analysis\n",
    "Based on the above analysis, we can find that there are very few audiophiles, accounting for only about 5% i.e. clusters 2 to 4. Cluster 5 is the mainstream users of the platform, which are light users. They listen to a very narrow range of music and only listen to it on average about 61 times, while 21.4% of the main users account for about one third of the plays and are characterised by not being more attracted to particular artists, as is the case with the light users. The remaining 5.4 per cent of users account for the other third, and they seem to have more of their own favourite styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Reconmandation \n",
    "\n",
    "According to the above analysis, facing light and mainstream users still give priority to similarity and breadth recommendation, so that they can find their favourite style and improve their platform stickiness.\n",
    "It is better to use collaborative filtering algorithm here.\n",
    "\n",
    "In the face of deep users, perhaps we can change the strategy, that is, based on the similarity of the song to make recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1  data processing\n",
    "\n",
    "First merge the duplicates. Because there is no apparent time series in this data, the duplicate columns are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(song_df))\n",
    "print(song_df.duplicated().sum())\n",
    "song_df = song_df.groupby(['song', 'title', 'artist_name', 'release', 'year', 'user'], observed=True)['play_count'].sum().reset_index()\n",
    "print(len(song_df))\n",
    "print(song_df.duplicated().sum())\n",
    "song_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# Logarithmic normalisation, dealing with extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model selection:\n",
    "\n",
    "Traditional music recommendation, is based on the similarity of users or items for recommendation, representative algorithms are collaborative filtering, or SVD, KNN, etc.\n",
    "\n",
    "For some complex patterns, such as the small percentage of users clustered above. It is common to take a hybrid recommendation approach that combine the advantages of multiple recommendation methods to generate more accurate and diverse recommendations.\n",
    "\n",
    "Compared to the above methods, deep models have been found to be more advantageous in capturing implicit and nonlinear relationships. Deep neural networks are able to learn complex non-linear relationships and better capture the implicit interaction patterns between users and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem faced, cold starts due to data sparsity. That is, how to solve it when faced with too little referenceable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the reasons above, we often encounter cold start, sparse data and other problems in recommender systems. Cold start is when a new user just joins with too little data, and it is difficult to get good recommendations based on playlists directly. The sparseness of the data, which may only account for 10% of all the user interactions with items in the data, leads to a lack of objectivity in the ratings. In this task, play_count as a user rating will result in the main recommendations being focused on very few items, as most items may only have 2 or 3 plays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, many people try to use GNN for recommendation, and I am also interested in it, so I practice it here, and according to the advantage of GNN is that the user's pattern can be fully recognised, as well as the relationship between the user and the song can be expressed in the model.\n",
    "Based on the analyses in 3.1.3, the number of songs, number of artists, number of albums, and average number of plays can be used as user characteristics.\n",
    "\n",
    "Unfortunately, GNN can't solve the cold start problem and is too computationally intensive I failed, it may perform well on the static song_datatsets dataset, and there are other ways to solve the sparsity problem, but it can't solve the two problems at the beginning, collaborative filtering is too simple, let's implement wide and deep\n",
    "\n",
    "Wide & Deep Learning is a recommender system model that combines the strengths of Linear Models (Wide) and Deep Neural Networks (Deep), first proposed by Google in 2016 for use in the Google Play app shop recommender system. Its goal is to both remember (Memorization) historical rules and generalise (Generalization) to new users or items to achieve efficient recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class WideAndDeepModel(nn.Module):\n",
    "    def __init__(self, num_users, num_songs, num_artists, num_releases, num_years,\n",
    "                 embedding_dim=64, hidden_layers=[256, 128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wide = nn.Linear(5, 1)\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.song_embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.artist_embedding = nn.Embedding(num_artists, embedding_dim)\n",
    "        self.release_embedding = nn.Embedding(num_releases, embedding_dim)\n",
    "        self.year_embedding = nn.Embedding(num_years, embedding_dim)\n",
    "    \n",
    "        deep_input_dim = embedding_dim * 5 + 1  \n",
    "        self.deep_layers = nn.ModuleList()\n",
    "        prev_dim = deep_input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            self.deep_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.deep_layers.append(nn.ReLU())\n",
    "            self.deep_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.deep_layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.final = nn.Linear(hidden_layers[-1] + 1, 1)\n",
    "        \n",
    "    def forward(self, wide_features, user_ids, song_ids, artist_ids, \n",
    "                release_ids, year_ids, play_count):\n",
    "        wide_out = self.wide(wide_features.float())\n",
    "        \n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        song_emb = self.song_embedding(song_ids)\n",
    "        artist_emb = self.artist_embedding(artist_ids)\n",
    "        release_emb = self.release_embedding(release_ids)\n",
    "        year_emb = self.year_embedding(year_ids)\n",
    "        \n",
    "        deep_input = torch.cat([\n",
    "            user_emb, song_emb, artist_emb, release_emb, year_emb,\n",
    "            play_count.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        deep_out = deep_input\n",
    "        for layer in self.deep_layers:\n",
    "            deep_out = layer(deep_out)\n",
    "        \n",
    "        final_input = torch.cat([deep_out, wide_out], dim=1)\n",
    "        output = self.final(final_input)\n",
    "        return output\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, df, is_training=True):\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.user_ids = torch.tensor(df['user_idx'].values, dtype=torch.long)\n",
    "        self.song_ids = torch.tensor(df['song_idx'].values, dtype=torch.long)\n",
    "        self.artist_ids = torch.tensor(df['artist_idx'].values, dtype=torch.long)\n",
    "        self.release_ids = torch.tensor(df['release_idx'].values, dtype=torch.long)\n",
    "        self.year_ids = torch.tensor(df['year_idx'].values, dtype=torch.long)\n",
    "        \n",
    "        self.wide_features = torch.tensor(\n",
    "            df[['user_idx', 'song_idx', 'artist_idx', 'release_idx', 'year_idx']].values,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        \n",
    "        self.play_count = torch.tensor(df['play_count_log'].values, dtype=torch.float)\n",
    "        \n",
    "        if is_training:\n",
    "            self.labels = torch.tensor(df['play_count_log'].values, dtype=torch.float)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_training:\n",
    "            return {\n",
    "                'wide_features': self.wide_features[idx],\n",
    "                'user_ids': self.user_ids[idx],\n",
    "                'song_ids': self.song_ids[idx],\n",
    "                'artist_ids': self.artist_ids[idx],\n",
    "                'release_ids': self.release_ids[idx],\n",
    "                'year_ids': self.year_ids[idx],\n",
    "                'play_count': self.play_count[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'wide_features': self.wide_features[idx],\n",
    "                'user_ids': self.user_ids[idx],\n",
    "                'song_ids': self.song_ids[idx],\n",
    "                'artist_ids': self.artist_ids[idx],\n",
    "                'release_ids': self.release_ids[idx],\n",
    "                'year_ids': self.year_ids[idx],\n",
    "                'play_count': self.play_count[idx]\n",
    "            }\n",
    "\n",
    "def preprocess_data(song_df):\n",
    "    le_user = LabelEncoder()\n",
    "    le_song = LabelEncoder()\n",
    "    le_artist = LabelEncoder()\n",
    "    le_release = LabelEncoder()\n",
    "    le_year = LabelEncoder()\n",
    "    \n",
    "    song_df['user_idx'] = le_user.fit_transform(song_df['user'])\n",
    "    song_df['song_idx'] = le_song.fit_transform(song_df['song'])\n",
    "    song_df['artist_idx'] = le_artist.fit_transform(song_df['artist_name'])\n",
    "    song_df['release_idx'] = le_release.fit_transform(song_df['release'])\n",
    "    song_df['year_idx'] = le_year.fit_transform(song_df['year'])\n",
    "    \n",
    "    song_df['play_count_log'] = np.log1p(song_df['play_count'])  # log1p = log(1+x)\n",
    "    \n",
    "    return song_df, le_user, le_song, le_artist, le_release, le_year\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, \n",
    "                patience=5, epochs=100, min_delta=1e-4):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                batch['wide_features'].to(device),\n",
    "                batch['user_ids'].to(device),\n",
    "                batch['song_ids'].to(device),\n",
    "                batch['artist_ids'].to(device),\n",
    "                batch['release_ids'].to(device),\n",
    "                batch['year_ids'].to(device),\n",
    "                batch['play_count'].to(device)\n",
    "            )\n",
    "            \n",
    "            loss = F.mse_loss(outputs.squeeze(), batch['labels'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model(\n",
    "                    batch['wide_features'].to(device),\n",
    "                    batch['user_ids'].to(device),\n",
    "                    batch['song_ids'].to(device),\n",
    "                    batch['artist_ids'].to(device),\n",
    "                    batch['release_ids'].to(device),\n",
    "                    batch['year_ids'].to(device),\n",
    "                    batch['play_count'].to(device)\n",
    "                )\n",
    "                loss = F.mse_loss(outputs.squeeze(), batch['labels'].to(device))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_user_recommendations(model, user_id, song_df, listened_songs, top_k=5):\n",
    "    \"\"\"\n",
    "    Generate song recommendations for a user\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        user_id: Target user ID\n",
    "        song_df: Song dataset\n",
    "        listened_songs: List of songs already listened to by the user\n",
    "        top_k: Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "        recommended_songs: List of recommended song IDs\n",
    "        scores: List of recommendation scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get unlistened songs\n",
    "        available_songs = song_df[~song_df['song'].isin(listened_songs)]\n",
    "        if available_songs.empty:\n",
    "            return [], []\n",
    "            \n",
    "        # Get unique song information\n",
    "        unique_songs = available_songs[['song_idx', 'song', 'artist_idx', 'release_idx', 'year_idx']].drop_duplicates('song_idx')\n",
    "        unique_songs = unique_songs.sort_values('song_idx').reset_index(drop=True)\n",
    "        num_songs = len(unique_songs)\n",
    "        \n",
    "        # Prepare features\n",
    "        user_ids = torch.full((num_songs,), user_id, dtype=torch.long, device=device)\n",
    "        song_ids = torch.tensor(unique_songs['song_idx'].values, dtype=torch.long, device=device)\n",
    "        artist_ids = torch.tensor(unique_songs['artist_idx'].values, dtype=torch.long, device=device)\n",
    "        release_ids = torch.tensor(unique_songs['release_idx'].values, dtype=torch.long, device=device)\n",
    "        year_ids = torch.tensor(unique_songs['year_idx'].values, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Batch processing\n",
    "        batch_size = 1024\n",
    "        all_scores = []\n",
    "        \n",
    "        for i in range(0, num_songs, batch_size):\n",
    "            batch_end = min(i + batch_size, num_songs)\n",
    "            batch_slice = slice(i, batch_end)\n",
    "            \n",
    "            # Wide features\n",
    "            wide_features = torch.cat([\n",
    "                user_ids[batch_slice].float().unsqueeze(1),\n",
    "                song_ids[batch_slice].float().unsqueeze(1),\n",
    "                artist_ids[batch_slice].float().unsqueeze(1),\n",
    "                release_ids[batch_slice].float().unsqueeze(1),\n",
    "                year_ids[batch_slice].float().unsqueeze(1)\n",
    "            ], dim=1)\n",
    "            \n",
    "            play_count = torch.zeros(batch_end - i, device=device)\n",
    "            \n",
    "            # Get predictions\n",
    "            scores = model(\n",
    "                wide_features,\n",
    "                user_ids[batch_slice],\n",
    "                song_ids[batch_slice],\n",
    "                artist_ids[batch_slice],\n",
    "                release_ids[batch_slice],\n",
    "                year_ids[batch_slice],\n",
    "                play_count\n",
    "            )\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        # Combine all batch scores\n",
    "        scores = torch.cat(all_scores, dim=0).squeeze()\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=min(top_k, len(scores)))\n",
    "        \n",
    "        # Get original song IDs\n",
    "        recommended_songs = unique_songs.iloc[top_k_indices.cpu().tolist()]['song'].values.tolist()\n",
    "        scores_list = top_k_scores.cpu().tolist()\n",
    "        \n",
    "        return recommended_songs, scores_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_user(song_df, train_ratio=0.8):\n",
    "    users = song_df['user_idx'].unique()\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for user in users:\n",
    "        user_data = song_df[song_df['user_idx'] == user]\n",
    "        n_items = len(user_data)\n",
    "        \n",
    "        n_train = max(int(n_items * train_ratio), 1)\n",
    "        n_train = min(n_train, n_items - 1) \n",
    "        \n",
    "        user_data = user_data.sample(frac=1, random_state=42)\n",
    "        \n",
    "        train_data.append(user_data.iloc[:n_train])\n",
    "        val_data.append(user_data.iloc[n_train:])\n",
    "    \n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    val_df = pd.concat(val_data, ignore_index=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "def main():\n",
    "    \"\"\"Main function to train and test the recommendation system\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    song_df = pd.read_csv('song_dataset.csv')\n",
    "    \n",
    "    print(\"Preprocessing data...\")\n",
    "    processed_df, le_user, le_song, le_artist, le_release, le_year = preprocess_data(song_df)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    train_df, val_df = split_data_by_user(processed_df, train_ratio=0.8)\n",
    "    \n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    train_dataset = MusicDataset(train_df)\n",
    "    val_dataset = MusicDataset(val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    model = WideAndDeepModel(\n",
    "        num_users=len(le_user.classes_),\n",
    "        num_songs=len(le_song.classes_),\n",
    "        num_artists=len(le_artist.classes_),\n",
    "        num_releases=len(le_release.classes_),\n",
    "        num_years=len(le_year.classes_)\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        patience=5,\n",
    "        epochs=100,\n",
    "        min_delta=1e-4\n",
    "    )\n",
    "    \n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model.state_dict(), 'music_recommender.pth')\n",
    "    \n",
    "    print(\"Saving label encoders...\")\n",
    "    with open('label_encoders.pkl', 'wb') as f:\n",
    "        pickle.dump((le_user, le_song, le_artist, le_release, le_year), f)\n",
    "    \n",
    "    print(\"\\nTesting recommendations...\")\n",
    "    user_idx = 0\n",
    "    listened_songs = processed_df[processed_df['user_idx'] == user_idx]['song'].unique()\n",
    "    recommended_songs, scores = get_user_recommendations(\n",
    "        model=model,\n",
    "        user_id=user_idx,\n",
    "        song_df=processed_df,\n",
    "        listened_songs=listened_songs,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(f\"\\nRecommendations for user {le_user.inverse_transform([user_idx])[0]}:\")\n",
    "    for song_id, score in zip(recommended_songs, scores):\n",
    "        song_info = song_df[song_df['song'] == song_id].iloc[0]\n",
    "        print(f\"Song: {song_info['title']} - {song_info['artist_name']}, Score: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is just part of the code that is deployed on streamlit. The url is xxxxxxxxx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import pandas as pd\n",
    "from model import WideAndDeepModel \n",
    "import pickle\n",
    "from model import get_user_recommendations\n",
    "\n",
    "# Fix file paths\n",
    "MODEL_PATH = \"music_recommender.pth\"\n",
    "DATA_PATH = \"song_dataset.csv\"\n",
    "ENCODERS_PATH = \"label_encoders.pkl\"\n",
    "@st.cache_resource\n",
    "def load_model_and_data():\n",
    "    try:\n",
    "        song_df = pd.read_csv(DATA_PATH)\n",
    "        \n",
    "        with open(ENCODERS_PATH, 'rb') as f:\n",
    "            le_dict = pickle.load(f)\n",
    "            \n",
    "        song_df['user_idx'] = le_dict[0].transform(song_df['user'])\n",
    "        song_df['song_idx'] = le_dict[1].transform(song_df['song'])\n",
    "        song_df['artist_idx'] = le_dict[2].transform(song_df['artist_name'])\n",
    "        song_df['release_idx'] = le_dict[3].transform(song_df['release'])\n",
    "        song_df['year_idx'] = le_dict[4].transform(song_df['year'].astype(str))\n",
    "        \n",
    "        model = WideAndDeepModel(\n",
    "            num_users=len(le_dict[0].classes_),\n",
    "            num_songs=len(le_dict[1].classes_),\n",
    "            num_artists=len(le_dict[2].classes_),\n",
    "            num_releases=len(le_dict[3].classes_),\n",
    "            num_years=len(le_dict[4].classes_)\n",
    "        )\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n",
    "        model.eval()\n",
    "        \n",
    "        return model, song_df, le_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"error: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_user_recommendations(model, user_id, song_df, listened_songs, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get unlistened songs\n",
    "        available_songs = song_df[~song_df['song'].isin(listened_songs)]\n",
    "        if available_songs.empty:\n",
    "            return [], []\n",
    "            \n",
    "        # Get unique song information\n",
    "        unique_songs = available_songs[['song_idx', 'song', 'artist_idx', 'release_idx', 'year_idx']].drop_duplicates('song_idx')\n",
    "        unique_songs = unique_songs.sort_values('song_idx').reset_index(drop=True)\n",
    "        num_songs = len(unique_songs)\n",
    "        \n",
    "        # Prepare features\n",
    "        user_ids = torch.full((num_songs,), user_id, dtype=torch.long)\n",
    "        song_ids = torch.tensor(unique_songs['song_idx'].values, dtype=torch.long)\n",
    "        artist_ids = torch.tensor(unique_songs['artist_idx'].values, dtype=torch.long)\n",
    "        release_ids = torch.tensor(unique_songs['release_idx'].values, dtype=torch.long)\n",
    "        year_ids = torch.tensor(unique_songs['year_idx'].values, dtype=torch.long)\n",
    "        \n",
    "        # Batch processing\n",
    "        batch_size = 1024\n",
    "        all_scores = []\n",
    "        \n",
    "        for i in range(0, num_songs, batch_size):\n",
    "            batch_end = min(i + batch_size, num_songs)\n",
    "            batch_slice = slice(i, batch_end)\n",
    "            \n",
    "            # Wide features\n",
    "            wide_features = torch.stack([\n",
    "                user_ids[batch_slice].float(),\n",
    "                song_ids[batch_slice].float(),\n",
    "                artist_ids[batch_slice].float(),\n",
    "                release_ids[batch_slice].float(),\n",
    "                year_ids[batch_slice].float()\n",
    "            ], dim=1)\n",
    "            \n",
    "            play_count = torch.zeros(batch_end - i)\n",
    "            \n",
    "            # Get predictions\n",
    "            scores = model(\n",
    "                wide_features,\n",
    "                user_ids[batch_slice],\n",
    "                song_ids[batch_slice],\n",
    "                artist_ids[batch_slice],\n",
    "                release_ids[batch_slice],\n",
    "                year_ids[batch_slice],\n",
    "                play_count\n",
    "            )\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        # Combine all batch scores\n",
    "        scores = torch.cat(all_scores, dim=0).squeeze()\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=min(top_k, len(scores)))\n",
    "        \n",
    "        # Get original song IDs and convert indices to list\n",
    "        indices = top_k_indices.cpu().tolist()\n",
    "        recommended_songs = unique_songs.iloc[indices]['song'].values.tolist()\n",
    "        scores_list = top_k_scores.cpu().tolist()\n",
    "        \n",
    "        return recommended_songs, scores_list\n",
    "def main():\n",
    "    st.title(\"ðŸŽµ Music Recommendation System\")\n",
    "    \n",
    "    # Load data and model\n",
    "    model, song_df, le_dict = load_model_and_data()\n",
    "    \n",
    "    if model is None or song_df is None or le_dict is None:\n",
    "        st.error(\"System initialization failed. Please ensure all required files exist.\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Create song selector\n",
    "    unique_songs = song_df[['title', 'artist_name']].drop_duplicates()\n",
    "    song_options = [f\"{row['title']} - {row['artist_name']}\" \n",
    "                   for _, row in unique_songs.iterrows()]\n",
    "    \n",
    "    selected_songs = st.multiselect(\n",
    "        \"Select songs you've listened to:\",\n",
    "        options=song_options,\n",
    "        max_selections=10\n",
    "    )\n",
    "    \n",
    "    if st.button(\"Get Recommendations\"):\n",
    "        if selected_songs:\n",
    "            with st.spinner(\"Generating recommendations...\"):\n",
    "                # Get song_ids for selected songs\n",
    "                listened_songs = []\n",
    "                for song in selected_songs:\n",
    "                    title = song.split(\" - \")[0]\n",
    "                    artist = song.split(\" - \")[1]\n",
    "                    song_id = song_df[(song_df['title'] == title) & \n",
    "                                    (song_df['artist_name'] == artist)]['song'].iloc[0]\n",
    "                    listened_songs.append(song_id)\n",
    "                \n",
    "                # Generate recommendations using the model\n",
    "                recommended_songs, scores = get_user_recommendations(\n",
    "                    model, \n",
    "                    user_id=0,  # Default ID for new users\n",
    "                    song_df=song_df,\n",
    "                    listened_songs=listened_songs,\n",
    "                    top_k=5  # Recommend 5 songs\n",
    "                )\n",
    "                \n",
    "                st.success(\"Recommendations generated!\")\n",
    "                \n",
    "                # Display recommendations\n",
    "                st.subheader(\"Recommended Songs:\")\n",
    "                for song_id, score in zip(recommended_songs, scores):\n",
    "                    song_info = song_df[song_df['song'] == song_id].iloc[0]\n",
    "                    with st.expander(f\"ðŸŽµ {song_info['title']} - {song_info['artist_name']}\"):\n",
    "                        col1, col2 = st.columns(2)\n",
    "                        with col1:\n",
    "                            st.write(f\"Artist: {song_info['artist_name']}\")\n",
    "                            st.write(f\"Album: {song_info['release']}\")\n",
    "                        with col2:\n",
    "                            st.write(f\"Year: {song_info['year']}\")\n",
    "                            st.write(f\"Match Score: {score:.2f}\")\n",
    "        else:\n",
    "            st.warning(\"Please select at least one song!\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center'>\n",
    "            <p>Music Recommendation System | Based on Wide & Deep Model</p>\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}