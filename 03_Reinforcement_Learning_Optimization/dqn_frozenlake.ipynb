{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Aberdeen\n",
    "\n",
    "## Applied AI (CS5079)\n",
    "\n",
    "### Resit CA1 - Reinforcement Learning with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library for environments\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "#Librairies to represent the output\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Essential libraries for computation\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_map = generate_random_map(size=10, p=0.3)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=random_map)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Describe, in detail, the following game elements using your own words. (2.5 marks): \n",
    "- **Observations:** \n",
    "In the FrozenLake-v1 environment, observations represent the agent's current position. The observation is an integer ranging from `0` to `nrow * ncol - 1`, where `nrow` and `ncol` are the number of rows and columns in the grid, respectively. For example, in a 4x4 grid, the observation range is `0` to `15`. The position is calculated as `current_row * ncol + current_col`, where the indices of rows and columns start from `0`.\n",
    "- **Action space:** \n",
    "The action space defines the actions the agent can take in the environment. In FrozenLake-v1, the action space is a discrete space consisting of four possible actions:  `0`: LEFT (move left),  `1`: DOWN (move down),  `2`: RIGHT (move right),  `3`: UP (move up).The agent chooses an action each time and tries to move to the corresponding direction.\n",
    "- **Reward:** The reward mechanism is used to guide the agent's learning process. In FrozenLake-v1, Reward schedule: Reach goal(G): `+1` ,Reach hole(H): `0` ,Reach frozen(F): `0`\n",
    "- **The environment‚Äôs info dictionary:** The environment's information dictionary provides additional information related to the current state. The information dictionary includes:  \n",
    "`prob`: The probability of transitioning to the next state after taking an action. \n",
    "- **Episode** An episode is a sequence of actions and state transitions starting from the initial state until a termination condition is met (e.g., reaching the goal or falling into a hole). The objective of each episode is to maximize the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Implement an agent based on a neural network, using a parameter ùúÄ, for making random moves, that decreases from 1 to 0.1. Describe how you deployed your agent, the motivation behind the design choices, and how you adjusted its parameters, going into detail on what each parameter does as well. You may use open-source code and libraries if you acknowledge them (15 marks). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Details\n",
    "\n",
    "Using **Deep Q-Network (DQN)** to implement the agent. DQN is a neural network-based reinforcement learning algorithm designed for discrete action spaces. To implement the Œµ-greedy policy, the agent will select a random action with probability Œµ and the estimated optimal action with probability (1-Œµ). Œµ will start at 1 and gradually decrease to 0.1 to balance exploration and exploitation.\n",
    "\n",
    "#### Design Choices\n",
    "\n",
    "- **Neural Network Architecture**: A simple fully connected network with the current state as a one-hot encoded input and Q-values for each action as output.\n",
    "- **Experience Replay**: A replay buffer is used to store past experiences, with random sampling for training to break data correlations.\n",
    "- **Target Network**: A fixed target network is used and updated periodically to stabilize the training process.\n",
    "- **Œµ-Greedy Policy**: Œµ starts at 1 and gradually decreases to 0.1, ensuring sufficient exploration in the early stages and more exploitation of learned strategies in later stages.\n",
    "\n",
    "#### Parameter Description\n",
    "\n",
    "- **Learning Rate (learning_rate)**: Controls the step size for updating neural network weights. A high value may lead to unstable training, while a low value results in slow training.\n",
    "- **Discount Factor (gamma)**: Determines the importance of future rewards. A value close to 1 focuses on long-term rewards, while a value close to 0 emphasizes immediate rewards.\n",
    "- **Buffer Size (buffer_size)**: The capacity of the experience replay buffer. A larger buffer can store more experiences, increasing sample diversity.\n",
    "- **Batch Size (batch_size)**: The number of experiences sampled from the buffer for each training step. Larger batch sizes improve training stability but increase computational cost.\n",
    "- **Epsilon Decay Rate (epsilon_decay)**: Controls the speed at which Œµ decays from its initial value to the minimum value.\n",
    "- **Target Network Update Frequency (target_update_freq)**: Specifies how often the target network is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining action constants\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "# Generates a random 10x10 map with a frozen tile probability of 0.3\n",
    "random_map = generate_random_map(size=10, p=0.3)\n",
    "print(\"Generated random map:\")\n",
    "for row in random_map:\n",
    "    print(row)\n",
    "\n",
    "# Initialize the environment\n",
    "env_dqn = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=True, render_mode='rgb_array')\n",
    "env_dqn.reset()\n",
    "\n",
    "# Get the state and action space size\n",
    "state_size = env_dqn.observation_space.n\n",
    "action_size = env_dqn.action_space.n\n",
    "\n",
    "# Defining the DQN Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Convert the state to one-hot encoding\n",
    "def one_hot(state, state_size):\n",
    "    vec = np.zeros(state_size)\n",
    "    vec[state] = 1.0\n",
    "    return vec\n",
    "\n",
    "# Defining the Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Defining Agents\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=128, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=1000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size, hidden_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        epsilon_threshold = self.epsilon_min + (self.epsilon - self.epsilon_min) * \\\n",
    "                            np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.epsilon = epsilon_threshold\n",
    "        self.steps_done += 1\n",
    "        if sample < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.policy_net(state)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add((state, action, reward, next_state, done))\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        experiences = self.buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Current Q value\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Target Q value\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q = rewards + (self.gamma * max_next_q * (1 - dones))\n",
    "        \n",
    "        # Calculating Losses\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Train the agent created in 1.2 on the game. Please note that a high number of episodes may be required for the agent to reach the goal depending on your implementation. Present the training process, the experiments (including the experimental setting), and discuss your results. You should make use of figures, including a line plot that shows how the average amount of rewards over episodes evolves over time (5 marks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3) Train the DQN agent and plot the training curve\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "target_update = 50\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Initialize the Agent\n",
    "agent_dqn = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training process\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    state = env_dqn.reset()[0]\n",
    "    state = one_hot(state, state_size)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent_dqn.select_action(state)\n",
    "        next_state, reward, done, _, info = env_dqn.step(action)\n",
    "        \n",
    "        # Calculate the distance from the current state to the target\n",
    "        current_row, current_col = divmod(np.argmax(state), env_dqn.ncol)\n",
    "        goal_row, goal_col = env_dqn.nrow - 1, env_dqn.ncol - 1\n",
    "        distance = np.sqrt((goal_row - current_row)**2 + (goal_col - current_col)**2)\n",
    "        max_distance = np.sqrt((goal_row)**2 + (goal_col)**2)\n",
    "        distance_reward = 1 - (distance / max_distance)\n",
    "        \n",
    "        # Total Rewards\n",
    "        if reward == 1.0:\n",
    "            total_reward += 1.0\n",
    "        total_reward += distance_reward\n",
    "        \n",
    "        # Transform the next state to one-hot\n",
    "        next_state_one_hot = one_hot(next_state, state_size)\n",
    "        \n",
    "        # Agent Learning\n",
    "        agent_dqn.step(state, action, distance_reward + reward, next_state_one_hot, done)\n",
    "        \n",
    "        state = next_state_one_hot\n",
    "    \n",
    "    rewards_per_episode.append(total_reward)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        agent_dqn.update_target_network()\n",
    "    \n",
    "    # Print progress\n",
    "    if episode % 100 == 0:\n",
    "        avg_reward = np.mean(rewards_per_episode[-100:])\n",
    "        print(f\"Episode {episode}/{num_episodes}, Average Reward: {avg_reward:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(agent_dqn.policy_net.state_dict(), \"dqn_frozenlake.pth\")\n",
    "\n",
    "# Plotting rewards during training\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(rewards_per_episode, label='Reward per Episode')\n",
    "# Calculating Moving Average\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards_per_episode, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, num_episodes), moving_avg, label=f'{window}-Episode Moving Average')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Agent Training Progress')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Randomly relying on the exploration of the grid can be a time-consuming process. Implement another agent, using ANY technique of your choice, which is aware of another additional information: The position of the chest at the bottom right corner. The agent should not cheat, i.e., they should not (at least initially) be aware of the layout of the map, i.e., the positions of the holes. Explain all design choices that were made to create this agent (20 marks). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementation Details and Design Choices**\n",
    "\n",
    "To improve the agent's exploration efficiency, we implemented the **A\\* search algorithm** as the agent's decision-making strategy. A\\* leverages a heuristic function (Manhattan distance) to guide the search direction, enabling faster convergence toward the target position. Although A\\* is typically used in scenarios with known maps, in this task, the agent begins with no knowledge of the map layout but updates its known information through incremental exploration.\n",
    "\n",
    "#### **Design Choices:**\n",
    "\n",
    "- **Heuristic Function**: Manhattan distance is used as the heuristic function to estimate the distance from the current state to the goal.\n",
    "- **Exploration Strategy**: Priority is given to exploring unknown areas to avoid falling into known traps.\n",
    "- **Information Update**: The agent updates its knowledge of the map after each step based on feedback, enabling better path planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.4) Implementing an A* agent\n",
    "\n",
    "# Import necessary libraries\n",
    "import heapq\n",
    "\n",
    "# Make sure the action constant is defined\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "class AStarAgent:\n",
    "    def __init__(self, env_astar):\n",
    "        self.env = env_astar\n",
    "        # Get the number of rows and columns\n",
    "        self.nrow, self.ncol = env_astar.desc.shape\n",
    "        self.goal = (self.nrow -1, self.ncol -1)\n",
    "        self.holes = set()\n",
    "        self.path = []\n",
    "    \n",
    "    def heuristic(self, a, b):\n",
    "        # Manhattan distance\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "    \n",
    "    def get_neighbors(self, state):\n",
    "        row, col = state\n",
    "        neighbors = []\n",
    "        actions = [LEFT, DOWN, RIGHT, UP]\n",
    "        for action in actions:\n",
    "            if action == LEFT:\n",
    "                new_col = max(col -1, 0)\n",
    "                new_row = row\n",
    "            elif action == DOWN:\n",
    "                new_row = min(row +1, self.nrow -1)\n",
    "                new_col = col\n",
    "            elif action == RIGHT:\n",
    "                new_col = min(col +1, self.ncol -1)\n",
    "                new_row = row\n",
    "            elif action == UP:\n",
    "                new_row = max(row -1, 0)\n",
    "                new_col = col\n",
    "            neighbors.append( (new_row, new_col) )\n",
    "        return neighbors\n",
    "    \n",
    "    def plan_path(self, start):\n",
    "        heap = []\n",
    "        heapq.heappush(heap, (0 + self.heuristic(start, self.goal), 0, start, []))\n",
    "        visited = set()\n",
    "        while heap:\n",
    "            est_total, cost, current, path = heapq.heappop(heap)\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            path = path + [current]\n",
    "            if current == self.goal:\n",
    "                return path\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                if neighbor in self.holes:\n",
    "                    continue\n",
    "                if neighbor not in visited:\n",
    "                    new_cost = cost +1\n",
    "                    est = new_cost + self.heuristic(neighbor, self.goal)\n",
    "                    heapq.heappush(heap, (est, new_cost, neighbor, path))\n",
    "        return []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if not self.path:\n",
    "            self.path = self.plan_path(state)\n",
    "            if not self.path:\n",
    "                return random.randint(0, self.env.action_space.n -1)  # Random Action\n",
    "            self.path.pop(0)  # Remove current state\n",
    "        if self.path:\n",
    "            next_state = self.path[0]\n",
    "            current = state\n",
    "            direction = None\n",
    "            if next_state[0] > current[0]:\n",
    "                direction = DOWN\n",
    "            elif next_state[0] < current[0]:\n",
    "                direction = UP\n",
    "            elif next_state[1] > current[1]:\n",
    "                direction = RIGHT\n",
    "            elif next_state[1] < current[1]:\n",
    "                direction = LEFT\n",
    "            self.path.pop(0)  # Remove the selected next state\n",
    "            return direction if direction is not None else random.randint(0, self.env.action_space.n -1)\n",
    "        return random.randint(0, self.env.action_space.n -1)\n",
    "    \n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()[0]\n",
    "        row, col = divmod(state, self.ncol)\n",
    "        current = (row, col)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = self.select_action(current)\n",
    "            next_state, reward, done, _, info = self.env.step(action)\n",
    "            next_row, next_col = divmod(next_state, self.ncol)\n",
    "            next_pos = (next_row, next_col)\n",
    "            # Recording cave locations\n",
    "            if self.env.desc[next_row][next_col] == b'H':\n",
    "                self.holes.add(next_pos)\n",
    "                self.path = []\n",
    "            # Calculate distance reward\n",
    "            distance = self.heuristic(next_pos, self.goal)\n",
    "            max_distance = self.heuristic((0,0), self.goal)\n",
    "            distance_reward = 1 - (distance / max_distance)\n",
    "            total_reward += distance_reward\n",
    "            if reward ==1.0:\n",
    "                total_reward +=1.0\n",
    "            current = next_pos\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Evaluate the agent created in 1.4 and discuss your results with respect to the previous questions (5 marks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.5) Evaluate the A* agent and plot the reward distribution\n",
    "\n",
    "# Creating a separate environment for the A* agent\n",
    "random_map_astar = generate_random_map(size=10, p=0.3)\n",
    "print(\"Generated random map (used by A* agent):\")\n",
    "for row in random_map_astar:\n",
    "    print(row)\n",
    "\n",
    "env_astar = gym.make(\"FrozenLake-v1\", desc=random_map_astar, is_slippery=True, render_mode='rgb_array')\n",
    "env_astar.reset()\n",
    "\n",
    "# Initializing the A* Agent\n",
    "agent_astar = AStarAgent(env_astar)  # Using a separate environment instance\n",
    "\n",
    "# Evaluation parameters\n",
    "num_episodes_astar = 1000\n",
    "rewards_per_episode_astar = []\n",
    "\n",
    "# Evaluation Process\n",
    "for episode in range(1, num_episodes_astar +1):\n",
    "    reward = agent_astar.run_episode()\n",
    "    rewards_per_episode_astar.append(reward)\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        avg_reward = np.mean(rewards_per_episode_astar[-100:])\n",
    "        print(f\"A* Episode {episode}/{num_episodes_astar}, Average Reward: {avg_reward:.2f}\")\n",
    "\n",
    "# Plotting the reward distribution of the A* agent\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(rewards_per_episode_astar, label='A* Agent Reward per Episode')\n",
    "# Calculating Moving Average\n",
    "window = 50\n",
    "moving_avg_astar = np.convolve(rewards_per_episode_astar, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, num_episodes_astar), moving_avg_astar, label=f'{window}-Episode Moving Average')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('A* Agent Training Progress')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6) Upload a video (maximum 2 minutes) of your best agent starting from the start position (at the top left) and reaching the goal (at the bottom right). If a link to the video is not provided, you will receive 0 marks for this sub-question (2.5 marks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.6.2) Record a video of the A* agent\n",
    "\n",
    "import heapq\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from IPython.display import Video, display\n",
    "\n",
    "# Defining action constants\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "# Generates a random 10x10 map with a frozen tile probability of 0.3\n",
    "random_map_astar = generate_random_map(size=10, p=0.3)\n",
    "print(\"Generated random map (used by A* agent):\")\n",
    "for row in random_map_astar:\n",
    "    print(row)\n",
    "\n",
    "# Initialize the A* agent's environment, specifying render_mode='rgb_array' to capture frames\n",
    "env_astar = gym.make(\"FrozenLake-v1\", desc=random_map_astar, is_slippery=True, render_mode='rgb_array')\n",
    "env_astar.reset()\n",
    "\n",
    "# Get the state and action space size\n",
    "state_size_astar = env_astar.observation_space.n\n",
    "action_size_astar = env_astar.action_space.n\n",
    "\n",
    "# Define the video recording function\n",
    "def record_astar_agent_video(env, agent, filename='astar_agent_video.avi', fps=4):\n",
    "    frames = []\n",
    "    state = env.reset()[0]\n",
    "    row, col = divmod(state, env.ncol)\n",
    "    current = (row, col)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Get the current frame\n",
    "        frame = env.render()  # Set render_mode='rgb_array' on initialization\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Agent chooses action\n",
    "        action = agent.select_action(current)\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        next_row, next_col = divmod(next_state, env.ncol)\n",
    "        next_pos = (next_row, next_col)\n",
    "        \n",
    "        # Recording cave locations\n",
    "        if env.desc[next_row][next_col] == b'H':\n",
    "            agent.holes.add(next_pos)\n",
    "            agent.path = []\n",
    "        \n",
    "        # Calculate distance reward\n",
    "        distance = agent.heuristic(next_pos, agent.goal)\n",
    "        max_distance = agent.heuristic((0,0), agent.goal)\n",
    "        distance_reward = 1 - (distance / max_distance)\n",
    "        total_reward += distance_reward\n",
    "        if reward ==1.0:\n",
    "            total_reward +=1.0\n",
    "        \n",
    "        # Update current status\n",
    "        current = next_pos\n",
    "    print(f\"A* Agent Total Reward: {total_reward}\")\n",
    "    env.close()\n",
    "    \n",
    "    # Save Video\n",
    "    if len(frames) > 0:\n",
    "        height, width, layers = frames[0].shape\n",
    "        video = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height))\n",
    "        for frame in frames:\n",
    "            # Convert RGB to BGR\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            video.write(frame_bgr)\n",
    "        video.release()\n",
    "        print(f\"A* Agent video saved as {filename}\")\n",
    "    else:\n",
    "        print(\"No frames were captured, A* Agent video was not generated.\")\n",
    "\n",
    "# Create a folder to save videos\n",
    "video_folder_astar_manual = 'videos_astar_manual'\n",
    "if not os.path.exists(video_folder_astar_manual):\n",
    "    os.makedirs(video_folder_astar_manual)\n",
    "\n",
    "# Define the video file path\n",
    "video_path_astar = os.path.join(video_folder_astar_manual, 'astar_agent_video.avi')\n",
    "\n",
    "# Recording Video\n",
    "record_astar_agent_video(env_astar, agent_astar, filename=video_path_astar, fps=4)\n",
    "\n",
    "# Displaying Videos in Jupyter Notebook\n",
    "if os.path.exists(video_path_astar):\n",
    "    display(Video(video_path_astar))\n",
    "else:\n",
    "    print(\"A* Agent video file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}